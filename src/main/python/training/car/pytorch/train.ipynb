{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "from torchvision.io import read_image\n",
    "import concurrent.futures\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torch.optim import RAdam\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import weighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "\tclasses = 0\n",
    "\tlabels = dict()\n",
    "\tdist = dict()\n",
    "\tdef __init__(self, annotations_file, img_dir):\n",
    "\t\tself.img_labels = pd.read_csv(annotations_file)\n",
    "\t\tself.img_dir = img_dir\n",
    "\t\ttempset = set()\n",
    "\t\tfor i in self.img_labels.iterrows():\n",
    "\t\t\ttempset.add(i[1][1])\n",
    "\t\t\tif i[1][1] in self.dist:\n",
    "\t\t\t\tself.dist[i[1][1]] += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.dist[i[1][1]] = 1\n",
    "\t\t\t\t\n",
    "\t\tself.labels = dict(zip(tempset, range(len(tempset))))\n",
    "\t\tself.classes = len(tempset)\n",
    "     \n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.img_labels)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\ttup = tuple(self.img_labels.iloc[idx])\n",
    "\t\timgloc = tup[2]\n",
    "\t\tlabelidx = self.labels[tup[1]]\n",
    "\t\timg_path = os.path.join(self.img_dir, imgloc)\n",
    "\t\timage = read_image(img_path)\n",
    "\n",
    "\t\treturn image, labelidx\n",
    "\n",
    "\tdef getLabel(self, idx):\n",
    "\t\ttup = tuple(self.img_labels.iloc[idx])\n",
    "\t\treturn tup[1]\n",
    "\n",
    "\tdef getDistributions(self):\n",
    "\t\treturn self.dist\n",
    "\n",
    "class TrDataset(Dataset):\n",
    "\tclasses = -1\n",
    "\tdef __init__(self, base_dataset, transformations):\n",
    "\t\tsuper(TrDataset, self).__init__()\n",
    "\t\tself.base = base_dataset\n",
    "\t\tself.transformations = transformations\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.base)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tx, y = self.base[idx]\n",
    "\t\treturn self.transformations(x), y\n",
    "\n",
    "\tdef getLabel(self, idx):\n",
    "\t\tx, y = self.base[idx]\n",
    "\t\treturn y\n",
    "\n",
    "\tdef getLabels(self):\n",
    "\t\tlabels = []\n",
    "\t\tfor x, y in self.base:\n",
    "\t\t\tlabels.append(y)\n",
    "\t\treturn labels\n",
    "\t\n",
    "\tdef getClassCount(self):\n",
    "\t\tif self.classes == -1:\n",
    "\t\t\ttempset = set()\n",
    "\t\t\tfor x, y in self.base:\n",
    "\t\t\t\ttempset.add(y)\n",
    "\t\t\tself.classes = len(tempset)\n",
    "\n",
    "\t\treturn self.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX WORKERS -- Change this to the number of workers you want to use for importing and loading the dataset\n",
    "MAX_WORKERS = 6\n",
    "# define training hyperparameters\n",
    "INIT_LR = 1e-5\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 400\n",
    "DECAY = 1e-4\n",
    "IMG_SIZE = 518\n",
    "# define the train and val splits\n",
    "TRAIN_SPLIT = 0.7\n",
    "VAL_SPLIT = 1 - TRAIN_SPLIT\n",
    "ANNOTATIONS = '$annotations.csv'\n",
    "\n",
    "# set the device we will be using to train the model\n",
    "device = torch.device(\"cuda\")\n",
    "# load the dataset\n",
    "print(\"[INFO] loading the dataset...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnno(file: str):\n",
    "\tcurLabel = []\n",
    "\tsplit = file.split('_')\n",
    "\ttry:\n",
    "\t\tcurLabel.append(split[2])\n",
    "\t\tcurLabel.append(split[0])\n",
    "\t\tcurLabel.append(split[1])\n",
    "\t\tcurLabel = '_'.join(curLabel)\n",
    "\texcept IndexError:\n",
    "\t\tprint(split)\n",
    "\treturn curLabel, file    \n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), 'exterior')\n",
    "\n",
    "labels = []\n",
    "files = os.listdir(input_dir)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "\tfutures = []\n",
    "\tfor file in files[:99]:\n",
    "\t\tif file.endswith('.jpg'):\n",
    "\t\t\tfutures.append(executor.submit(getAnno, file))\n",
    "\tfor future in concurrent.futures.as_completed(futures):\n",
    "\t\ttemp = future.result()\n",
    "\t\tlabels.append((temp[0], temp[1]))\n",
    "\n",
    "\n",
    "with open(ANNOTATIONS, 'w', newline='') as file:\n",
    "\tlabels = np.array(labels)\n",
    "\tdf = pd.DataFrame(labels)\n",
    "\tdf.to_csv(file)\n",
    "\tprint('[INFO] Annotations saved to file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CustomImageDataset(ANNOTATIONS, input_dir)\n",
    "vaTransforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "\ttransforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "trTransforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "\ttransforms.ToPILImage(),\n",
    "    transforms.RandomApply(nn.ModuleList([\n",
    "        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.5, 1.0)),\n",
    "\t\ttransforms.RandomHorizontalFlip(),\n",
    "\t\ttransforms.RandomVerticalFlip(),\n",
    "\t\ttransforms.RandomAffine(degrees=(30, 70), translate=(0.0, 0.1), scale=(0.5, 0.7)),\n",
    "\t\ttransforms.RandomPerspective(),\n",
    "\t\ttransforms.ColorJitter(brightness=(0.5, 1), contrast=(0.5, 1), saturation=(0.5, 1), hue=(0, 0))\n",
    "\t])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "classes = data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide train and test data\n",
    "print(\"[INFO] Generating the train/test split...\")\n",
    "numTrainSamp = int(len(data) * 0.9)\n",
    "numTestSamp = int(len(data) * 0.1)\n",
    "\n",
    "while numTrainSamp + numTestSamp != len(data):\n",
    "\tnumTrainSamp += 1\n",
    " \n",
    "print(\"[INFO] Test set size:\", numTestSamp)\n",
    "\n",
    "labels = data.getDistributions()\n",
    "\n",
    "(data, testData) = random_split(data,\n",
    "\t[numTrainSamp, numTestSamp],\n",
    "\tgenerator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# calculate the train/validation split\n",
    "print(\"[INFO] Generating the train/validation split\")\n",
    "numTrainSamples = int(len(data) * TRAIN_SPLIT)\n",
    "numValSamples = int(len(data) * VAL_SPLIT)\n",
    "\n",
    "while numTrainSamples + numValSamples != len(data):\n",
    "\tnumTrainSamples += 1\n",
    "\n",
    "print(\"[INFO] Validation set size:\", numValSamples, \"Train set size:\", numTrainSamples)\n",
    "\n",
    "(data, valData) = random_split(data,\n",
    "\t[numTrainSamples, numValSamples],\n",
    "\tgenerator=torch.Generator().manual_seed(42))\n",
    "\n",
    "trainData = TrDataset(data, trTransforms)\n",
    "valData = TrDataset(valData, vaTransforms)\n",
    "testData = TrDataset(testData, vaTransforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating class weights for training dataset\n",
    "print(\"[INFO] Generating class weights for training dataset...\")\n",
    "weights = weighter.make_weights(trainData, trainData.getClassCount(), BATCH_SIZE, MAX_WORKERS)\n",
    "samplerTrain = WeightedRandomSampler(weights=weights.double(), num_samples=len(trainData))\n",
    "\n",
    "# weights = weighter.make_weights(valData, valData.getClassCount(), BATCH_SIZE, MAX_WORKERS)\n",
    "# samplerVal = WeightedRandomSampler(weights=weights, num_samples=len(valData))\n",
    "\n",
    "# with open('ClassDistCurrent.csv', 'w', newline='') as file:\n",
    "# \tdf = pd.DataFrame([labels])\n",
    "# \tdf.to_csv(file, index=False)\n",
    "# \tprint('[INFO] Distributions saved to file')\n",
    "\n",
    "print(\"[INFO] EPOCHS set to\", EPOCHS)\n",
    "print(\"[INFO] BATCH_SIZE set to\", BATCH_SIZE)\n",
    "\n",
    "# initialize the train, validation, and test data loaders\n",
    "trainDataLoader = DataLoader(trainData, batch_size=BATCH_SIZE, num_workers=MAX_WORKERS, pin_memory=True, sampler=samplerTrain)\n",
    "valDataLoader = DataLoader(valData, batch_size=BATCH_SIZE, num_workers=MAX_WORKERS, pin_memory=True)\n",
    "testDataLoader = DataLoader(testData, batch_size=BATCH_SIZE, num_workers=MAX_WORKERS, pin_memory=True)\n",
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(trainDataLoader.dataset)\n",
    "valSteps = len(valDataLoader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the LeNet model\n",
    "print(\"[INFO] initializing the ImageNet model...\")\n",
    "# model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT).to(device)\n",
    "model = model = torch.hub.load(\"facebookresearch/swag\", model=\"vit_h14_in1k\").to(device)\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "# initialize our optimizer and loss function\n",
    "opt = RAdam(model.parameters(recurse=True), lr=INIT_LR, weight_decay=DECAY)\n",
    "lossFn = nn.NLLLoss()\n",
    "\n",
    "# initialize a dictionary to store training history\n",
    "H = {\n",
    "\t\"train_loss\": [],\n",
    "\t\"train_acc\": [],\n",
    "\t\"val_loss\": [],\n",
    "\t\"val_acc\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure how long training is going to take\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "\n",
    "# loop over our epochs\n",
    "looptimes = []\n",
    "for e in range(0, EPOCHS):\n",
    "\tloopTime = time.time()\n",
    "\t# set the model in training mode\n",
    "\tmodel.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalValLoss = 0\n",
    "\t# initialize the number of correct predictions in the training\n",
    "\t# and validation step\n",
    "\ttrainCorrect = 0\n",
    "\tvalCorrect = 0\n",
    "\t# loop over the training set\n",
    "\tfor (x, y) in trainDataLoader:\n",
    "\t\t(x, y) = (x.to(device), y.to(device))\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\toutput = model(x)\n",
    "\t\tloss1 = lossFn(output, y)\n",
    "\t\tloss = loss1\n",
    "\t\t# zero out the gradients, perform the backpropagation step,\n",
    "\t\t# and update the weights\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\t\t# add the loss to the total training loss so far and\n",
    "\t\t# calculate the number of correct predictions\n",
    "\t\ttotalTrainLoss += loss\n",
    "\t\t_, preds = torch.max(output, 1)\n",
    "\t\ttrainCorrect += torch.sum(preds == y)\n",
    "  \n",
    "  # switch off autograd for evaluation\n",
    "\twith torch.no_grad():\n",
    "\t\t# set the model in evaluation mode\n",
    "\t\tmodel.eval()\n",
    "\t\t# loop over the validation set\n",
    "\t\tfor (x, y) in valDataLoader:\n",
    "\t\t\t# send the input to the device\n",
    "\t\t\t(x, y) = (x.to(device), y.to(device))\n",
    "\t\t\t# make the predictions and calculate the validation loss\n",
    "\t\t\toutput = model(x)\n",
    "\t\t\tloss = lossFn(output, y)\n",
    "\t\t\t_, preds = torch.max(output, 1)\n",
    "\t\t\ttotalValLoss += loss\n",
    "\t\t\t# calculate the number of correct predictions\n",
    "\t\t\tvalCorrect += torch.sum(preds == y)\n",
    "   \n",
    "\t# calculate the average training and validation loss\n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\tavgValLoss = totalValLoss / valSteps\n",
    "\t# calculate the training and validation accuracy\n",
    "\ttrainCorrect = trainCorrect / trainSteps\n",
    "\tvalCorrect = valCorrect / valSteps\n",
    "  \n",
    "\t# update our training history\n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"train_acc\"].append(trainCorrect.cpu().detach().numpy())\n",
    "\tH[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "\tH[\"val_acc\"].append(valCorrect.cpu().detach().numpy())\n",
    "\t# print the model training and validation information\n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
    "\tprint(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
    "\t\tavgTrainLoss, trainCorrect))\n",
    "\tprint(\"Val loss: {:.6f}, Val accuracy: {:.4f}\".format(\n",
    "\t\tavgValLoss, valCorrect))\n",
    "\tprint(\"Loop time: {:.2f}s\".format(time.time() - loopTime))\n",
    "\tprint(\"Total time: {:.2f} minutes\\n\".format((time.time() - startTime) / 60))\n",
    "\tlooptimes.append(time.time() - loopTime)\n",
    "\n",
    "\tprint(\"Estimated time remaining: {:.2f} minutes\\n\".format(((sum(looptimes) / len(looptimes)) * (EPOCHS - e)) / 60))\n",
    "# finish measuring how long training took\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "\tendTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now evaluate the network on the test set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "# turn off autograd for testing evaluation\n",
    "with torch.no_grad():\n",
    "\t# set the model in evaluation mode\n",
    "\tmodel.eval()\n",
    "\t\n",
    "\t# initialize a list to store our predictions\n",
    "\tpreds = []\n",
    "\ttestCorrect = 0\n",
    "\t# loop over the test set\n",
    "\tfor (x, y) in testDataLoader:\n",
    "\t\t# send the input to the device\n",
    "\t\tx = x.to(device)\n",
    "\t\ty = y.to(device)\n",
    "  \n",
    "\t\t# make the predictions and add them to the list\n",
    "\t\toutput = model(x)\n",
    "\t\t_, preds = torch.max(output, 1)\n",
    "\t\t# calculate the number of correct predictions\n",
    "\t\ttestCorrect += torch.sum(preds == y.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a classification report\n",
    "print(\"[INFO] Accuracy on {:d} classes: {:.2f}%\".format(classes, (testCorrect / len(testDataLoader.dataset)) * 100))\n",
    "\n",
    "modelFile = Path(os.getcwd()).parents[0]\n",
    "modelFile = os.path.join(modelFile, 'models', 'car_model_p1', 'car_model.pt')\n",
    "plotFile = Path(os.getcwd()).parents[0]\n",
    "plotFile = os.path.join(plotFile, 'models', 'car_model_p1', 'plot.png')\n",
    "i = 1\n",
    "\n",
    "while os.path.exists(Path(modelFile).parents[0]):\n",
    "\ti += 1 \n",
    "\tmodelFile = os.path.join(Path(os.getcwd()).parents[0], 'models', 'car_model_p{:d}'.format(i), 'car_model.pt')\n",
    "\tplotFile = os.path.join(Path(os.getcwd()).parents[0], 'models', 'car_model_p{:d}'.format(i), 'plot.png')\n",
    "\n",
    "if not os.path.exists(Path(modelFile).parents[1]): os.mkdir(Path(modelFile).parents[1])\n",
    "os.mkdir(Path(modelFile).parents[0])\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(H[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(H[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(plotFile)\n",
    "# serialize the model to disk\n",
    "torch.save(model, modelFile)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
